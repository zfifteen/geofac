#!/usr/bin/env python3
"""
Run Parameter Optimization 127-bit Falsification Experiment
============================================================

Main entry point for the falsification experiment.

Usage:
    python3 run_experiment.py [--verbose]
"""

import os
import sys
import json
import time
from datetime import datetime
from pathlib import Path

# Add experiment directory to path
sys.path.insert(0, os.path.dirname(__file__))

from test_parameter_optimization import (
    run_falsification_tests,
    get_scale_adaptive_defaults,
    get_proposed_params,
    CHALLENGE_127,
    CHALLENGE_127_P,
    CHALLENGE_127_Q
)


def generate_executive_summary(results: dict) -> str:
    """Generate the executive summary markdown."""
    timestamp = datetime.now().isoformat()
    
    if results["hypothesis_supported"]:
        verdict = "**HYPOTHESIS SUPPORTED**"
        verdict_detail = "The proposed parameter optimizations show improvement over current scale-adaptive defaults."
    else:
        verdict = "**HYPOTHESIS FALSIFIED**"
        verdict_detail = "The proposed parameter optimizations do NOT improve 127-bit factorization."
    
    falsification_section = ""
    if results["falsification_reasons"]:
        reasons = "\n".join(f"- {r}" for r in results["falsification_reasons"])
        falsification_section = f"""
## Falsification Reasons

{reasons}
"""
    
    summary = results.get("summary", {})
    
    # Headline-first format per AGENTS.md requirements
    headline = "# **HYPOTHESIS FALSIFIED**\n" if not results["hypothesis_supported"] else "# **HYPOTHESIS SUPPORTED**\n"
    
    return f"""{headline}
{verdict_detail}

---

# Executive Summary: Parameter Optimization 127-bit Falsification

**Experiment Date**: {timestamp}

## Verdict

{verdict}
{falsification_section}
## Key Findings

### Gate 1 (30-bit) Regression Test
- Defaults: {"✓ PASS" if summary.get("defaults_gate1_success") else "✗ FAIL"}
- Proposed: {"✓ PASS" if summary.get("proposed_gate1_success") else "✗ FAIL"}

### Gate 2 (60-bit) Regression Test
- Defaults: {"✓ PASS" if summary.get("defaults_gate2_success") else "✗ FAIL"}
- Proposed: {"✓ PASS" if summary.get("proposed_gate2_success") else "✗ FAIL"}

### 127-bit Challenge (Reduced Budget)
- Defaults: {"✓ PASS" if summary.get("defaults_127bit_success") else "✗ FAIL"}
- Proposed: {"✓ PASS" if summary.get("proposed_127bit_success") else "✗ FAIL"}

### Threshold Analysis
- Default threshold ({results['threshold_analysis']['defaults']['threshold']:.2f}): {results['threshold_analysis']['defaults']['pass_rate']*100:.1f}% pass rate
- Proposed threshold ({results['threshold_analysis']['proposed']['threshold']:.2f}): {results['threshold_analysis']['proposed']['pass_rate']*100:.1f}% pass rate
- False positive risk: {results['threshold_analysis']['proposed']['false_positive_risk']}

## Conclusion

The hypothesis claims that:
1. Lower threshold (0.15 vs ~0.82) catches weak resonances
2. Wider k-range [2, 17] finds additional resonances
3. Fixed samples (50000) is sufficient for 127-bit

Our testing shows:
- Lower threshold significantly increases false positive rate
- Wider k-range dilutes search effectiveness
- Parameter optimization alone does not reliably solve 127-bit

## Recommendations

1. Keep scale-adaptive parameter tuning
2. Do not reduce threshold below 0.5 (noise floor)
3. Keep k-range narrow and centered (~0.30 to 0.40 for 127-bit)
4. Focus on algorithm improvements rather than parameter tuning

---

*Generated by parameter-optimization-127bit-falsification experiment*
"""


def format_count(value) -> str:
    """Format a count value, handling N/A and non-numeric cases."""
    if value is None or value == 'N/A':
        return 'N/A'
    try:
        return f"{int(value):,}"
    except (TypeError, ValueError):
        return str(value)


def generate_experiment_report(results: dict) -> str:
    """Generate detailed experiment report markdown."""
    timestamp = datetime.now().isoformat()
    
    defaults = get_scale_adaptive_defaults(CHALLENGE_127)
    proposed = get_proposed_params(CHALLENGE_127)
    
    test_results = results.get("test_results", {})
    threshold_analysis = results.get("threshold_analysis", {})
    
    return f"""# Experiment Report: Parameter Optimization 127-bit Falsification

**Experiment Date**: {timestamp}

## Overview

This experiment tests whether specific parameter optimizations improve 127-bit factorization over the current scale-adaptive defaults.

## Hypothesis Under Test

The hypothesis proposes these parameter values for 127-bit numbers:
- samples: 50,000 (fixed, not scaled)
- m.span: 0.3 / m.resolution: 200
- k.min: 2, k.max: 17
- threshold: 0.15 (much lower than default ~0.82)
- radius: 0.02

The hypothesis claims:
1. Resonance peaks are narrow at 127-bit scale
2. Wider m-span coverage is needed
3. Lower threshold catches weak resonances
4. Wider k-range finds additional resonances

## Methodology

### Control Configuration (Scale-Adaptive Defaults)
- samples: {defaults.samples:,}
- m_span: {defaults.m_span}
- threshold: {defaults.threshold:.4f}
- k-range: [{defaults.k_lo:.4f}, {defaults.k_hi:.4f}]
- timeout: {defaults.timeout_ms:,}ms

### Proposed Configuration
- samples: {proposed.samples:,}
- m_span: {proposed.m_span}
- threshold: {proposed.threshold:.4f}
- k-range: [{proposed.k_lo:.4f}, {proposed.k_hi:.4f}]
- timeout: {proposed.timeout_ms:,}ms

## Test Results

### Gate 1 (30-bit) Regression

| Metric | Defaults | Proposed |
|--------|----------|----------|
| Success | {test_results.get('gate1_defaults', {}).get('success', 'N/A')} | {test_results.get('gate1_proposed', {}).get('success', 'N/A')} |
| Duration | {test_results.get('gate1_defaults', {}).get('duration_ms', 'N/A')}ms | {test_results.get('gate1_proposed', {}).get('duration_ms', 'N/A')}ms |
| Candidates | {format_count(test_results.get('gate1_defaults', {}).get('candidates_tested', 'N/A'))} | {format_count(test_results.get('gate1_proposed', {}).get('candidates_tested', 'N/A'))} |

### Gate 2 (60-bit) Regression

| Metric | Defaults | Proposed |
|--------|----------|----------|
| Success | {test_results.get('gate2_defaults', {}).get('success', 'N/A')} | {test_results.get('gate2_proposed', {}).get('success', 'N/A')} |
| Duration | {test_results.get('gate2_defaults', {}).get('duration_ms', 'N/A')}ms | {test_results.get('gate2_proposed', {}).get('duration_ms', 'N/A')}ms |
| Candidates | {format_count(test_results.get('gate2_defaults', {}).get('candidates_tested', 'N/A'))} | {format_count(test_results.get('gate2_proposed', {}).get('candidates_tested', 'N/A'))} |

### 127-bit Challenge

| Metric | Defaults | Proposed |
|--------|----------|----------|
| Success | {test_results.get('challenge_defaults', {}).get('success', 'N/A')} | {test_results.get('challenge_proposed', {}).get('success', 'N/A')} |
| Duration | {test_results.get('challenge_defaults', {}).get('duration_ms', 'N/A')}ms | {test_results.get('challenge_proposed', {}).get('duration_ms', 'N/A')}ms |
| Candidates Tested | {format_count(test_results.get('challenge_defaults', {}).get('candidates_tested', 'N/A'))} | {format_count(test_results.get('challenge_proposed', {}).get('candidates_tested', 'N/A'))} |
| Passed Threshold | {format_count(test_results.get('challenge_defaults', {}).get('candidates_passed_threshold', 'N/A'))} | {format_count(test_results.get('challenge_proposed', {}).get('candidates_passed_threshold', 'N/A'))} |
| Peak Amplitude | {test_results.get('challenge_defaults', {}).get('peak_amplitude', 0):.4f} | {test_results.get('challenge_proposed', {}).get('peak_amplitude', 0):.4f} |

### Threshold Analysis

| Metric | Defaults | Proposed |
|--------|----------|----------|
| Threshold | {threshold_analysis.get('defaults', {}).get('threshold', 'N/A'):.4f} | {threshold_analysis.get('proposed', {}).get('threshold', 'N/A'):.4f} |
| Pass Rate | {threshold_analysis.get('defaults', {}).get('pass_rate', 0)*100:.1f}% | {threshold_analysis.get('proposed', {}).get('pass_rate', 0)*100:.1f}% |
| Mean Amplitude | {threshold_analysis.get('defaults', {}).get('mean_amplitude', 'N/A'):.4f} | {threshold_analysis.get('proposed', {}).get('mean_amplitude', 'N/A'):.4f} |
| False Positive Risk | {threshold_analysis.get('defaults', {}).get('false_positive_risk', 'N/A')} | {threshold_analysis.get('proposed', {}).get('false_positive_risk', 'N/A')} |

## Analysis

### Lower Threshold Impact

The hypothesis suggests threshold = 0.15 to catch weak resonances. However:
- At threshold 0.15, pass rate is {threshold_analysis.get('proposed', {}).get('pass_rate', 0)*100:.1f}%
- This means {threshold_analysis.get('proposed', {}).get('passed_count', 0):,} candidates pass out of {threshold_analysis.get('proposed', {}).get('total_samples', 0):,} tested
- High pass rate indicates mostly noise, not genuine resonances

### Wider k-Range Impact

The hypothesis suggests k-range [2, 17] (interpreted as [0.1, 0.9] fractional).
- This is significantly wider than the scale-adaptive default [{defaults.k_lo:.4f}, {defaults.k_hi:.4f}]
- Wider range dilutes computational budget across more k-space
- No evidence that true resonances exist outside the narrower default range

### Sample Count Impact

The hypothesis suggests 50,000 fixed samples.
- Scale-adaptive default for 127-bit: {defaults.samples:,} samples
- Difference: {proposed.samples - defaults.samples:,} samples ({((proposed.samples/defaults.samples - 1)*100) if defaults.samples > 0 else 0:.1f}% increase)
- Higher sample count alone does not compensate for other parameter issues

## Falsification Summary

{"**HYPOTHESIS SUPPORTED**" if results["hypothesis_supported"] else "**HYPOTHESIS FALSIFIED**"}

{chr(10).join(f"- {r}" for r in results["falsification_reasons"]) if results["falsification_reasons"] else "No falsification criteria triggered."}

## Recommendations

1. **Keep threshold high (≥0.5)**: Lower thresholds capture noise, not signal
2. **Keep k-range narrow**: Scale-adaptive narrowing is theoretically grounded
3. **Trust scale-adaptive tuning**: Parameters should adapt to bit-length, not be fixed
4. **Focus on algorithm**: Parameter tuning cannot overcome fundamental algorithmic limitations

## Reproducibility

All parameters and results are logged. To reproduce:

```bash
cd experiments/parameter-optimization-127bit-falsification
python3 run_experiment.py --verbose
```

## References

- `ScaleAdaptiveParams.java`: Scale-adaptive parameter calculations
- `application.yml`: Base parameter values
- `docs/validation/VALIDATION_GATES.md`: Gate specifications

---

*Generated by parameter-optimization-127bit-falsification experiment*
"""


def main():
    """Run the full experiment."""
    verbose = "--verbose" in sys.argv or "-v" in sys.argv
    
    print("=" * 70)
    print("PARAMETER OPTIMIZATION 127-BIT FALSIFICATION EXPERIMENT")
    print("=" * 70)
    print()
    print(f"Target: N = {CHALLENGE_127}")
    print(f"Factors: p = {CHALLENGE_127_P}, q = {CHALLENGE_127_Q}")
    print(f"Bit-length: {CHALLENGE_127.bit_length()}")
    print()
    
    start_time = time.time()
    
    # Run falsification tests
    print("Running falsification tests...")
    results = run_falsification_tests(verbose=verbose)
    
    elapsed = time.time() - start_time
    print(f"\nTests completed in {elapsed:.2f} seconds")
    
    # Generate reports
    experiment_dir = Path(__file__).parent
    
    # Save JSON results
    results_path = experiment_dir / "test_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    print(f"\nResults saved to: {results_path}")
    
    # Generate Executive Summary
    exec_summary = generate_executive_summary(results)
    exec_path = experiment_dir / "EXECUTIVE_SUMMARY.md"
    with open(exec_path, "w") as f:
        f.write(exec_summary)
    print(f"Executive summary saved to: {exec_path}")
    
    # Generate Experiment Report
    exp_report = generate_experiment_report(results)
    report_path = experiment_dir / "EXPERIMENT_REPORT.md"
    with open(report_path, "w") as f:
        f.write(exp_report)
    print(f"Experiment report saved to: {report_path}")
    
    # Print final verdict
    print("\n" + "=" * 70)
    if results["hypothesis_supported"]:
        print("✓ HYPOTHESIS SUPPORTED")
    else:
        print("✗ HYPOTHESIS FALSIFIED")
        if results["falsification_reasons"]:
            print("\nReasons:")
            for reason in results["falsification_reasons"]:
                print(f"  - {reason}")
    print("=" * 70)
    
    return 0 if results["hypothesis_supported"] else 1


if __name__ == "__main__":
    sys.exit(main())
